# -*- coding: utf-8 -*-
"""againnnRNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10r3vjdL56J0pLFLSEzblnyjNiif4aCrb
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import numpy as np
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv('/content/DLGoldData.csv')  # Replace 'your_data.csv' with your actual file path

# Set your feature columns and target column
features = data.drop(columns=['Date','Onion Price']).values  # All columns except the target
target = data['Gold Price'].values.reshape(-1, 1)    # Target column

scaler_x = MinMaxScaler()
scaler_y = MinMaxScaler()

# Fit scalers on the features and target
features_scaled = scaler_x.fit_transform(features)
target_scaled = scaler_y.fit_transform(target)

def create_sequences(features, target, seq_length=30):
    X = []
    y = []
    for i in range(seq_length, len(features)):
        X.append(features[i-seq_length:i])
        y.append(target[i])
    return np.array(X), np.array(y)

# Create sequences with a specified sequence length (e.g., 30)
seq_length = 30
X, y = create_sequences(features_scaled, target_scaled, seq_length)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

!pip install keras-tuner

import keras_tuner as kt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.preprocessing import MinMaxScaler
import numpy as np

def build_model(hp):
    model = Sequential()

    # First RNN layer
    model.add(SimpleRNN(
        units=hp.Int('units_1', min_value=50, max_value=150, step=50),
        return_sequences=True,
        input_shape=(X_train.shape[1], X_train.shape[2])
    ))
    model.add(Dropout(hp.Float('dropout_1', 0.1, 0.5, step=0.1)))

    # Second RNN layer
    model.add(SimpleRNN(units=hp.Int('units_2', min_value=50, max_value=100, step=50), return_sequences=True))
    model.add(Dropout(hp.Float('dropout_2', 0.1, 0.5, step=0.1)))

    # Third RNN layer (without return_sequences)
    model.add(SimpleRNN(units=hp.Int('units_3', min_value=25, max_value=75, step=25)))
    model.add(Dropout(hp.Float('dropout_3', 0.1, 0.5, step=0.1)))

    # Batch Normalization Layer
    model.add(BatchNormalization())

    # Dense output layer
    model.add(Dense(units=1))

    # Compile model
    model.compile(
        optimizer=Adam(learning_rate=hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='mean_squared_error'
    )
    return model

# Initialize the Keras Tuner
tuner = kt.RandomSearch(
    build_model,
    objective='val_loss',
    max_trials=5,  # Increase if you have more computational resources
    executions_per_trial=2,  # Increase for stability in results
    directory='rnn_tuning',
    project_name='gold_price_prediction'
)

# Early Stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Perform the search
tuner.search(X_train, y_train,
             epochs=50,
             validation_split=0.2,
             batch_size=32,
             callbacks=[early_stopping])  # Add EarlyStopping callback

# Get the best model
best_model = tuner.get_best_models(num_models=1)[0]

# Retrieve the best model after hyperparameter tuning
best_model = tuner.get_best_models(num_models=1)[0]

# Now, use this best model for training
history = best_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), verbose=1)

plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Predictions on test data
predictions = best_model.predict(X_test)

# Reshape predictions to match the scaler requirements
if predictions.ndim == 3:
    predictions = predictions.reshape(predictions.shape[0], predictions.shape[2])

# Inverse transform the predictions and actual values
predictions_inverse = scaler_y.inverse_transform(predictions)
y_test_inverse = scaler_y.inverse_transform(y_test)

# Flatten to remove any additional dimensions
predictions_inverse = predictions_inverse.flatten()
y_test_inverse = y_test_inverse.flatten()

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

mae = mean_absolute_error(y_test_inverse, predictions_inverse)
# Calculate Mean Absolute Percentage Error (MAPE)
mape = np.mean(np.abs((y_test_inverse - predictions_inverse) / y_test_inverse)) * 100
# Calculate R-squared (R²) as an alternative to accuracy in regression
r2 = r2_score(y_test_inverse, predictions_inverse)

print(f'Mean Absolute Error (MAE) of RNN: {mae}')
print(f'Mean Absolute Percentage Error (MAPE) of RNN: {mape}%')
print(f'R-squared (R²) of RNN: {r2}')

plt.figure(figsize=(12, 6))
plt.plot(y_test_inverse, label='Actual', color='blue')
plt.plot(predictions_inverse, label='Predicted', color='red', linestyle='--')
plt.title('Actual vs Predicted Values for RNN')
plt.xlabel('Sample Index')
plt.ylabel('Target Value')
plt.legend()
plt.show()