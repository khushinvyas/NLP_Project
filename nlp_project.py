# -*- coding: utf-8 -*-
"""NLP_PROJECT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EQYNeVA4L5BKnABNPT61WjDthiMFZUn2
"""

!pip install flask pyngrok
!ngrok config add-authtoken 2o6OP9GQLe9H4tY2WBKkxAx79Wr_22jQPQ3KXah3NMWgfWFiT
!pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
!pip install --no-deps xformers trl peft accelerate bitsandbytes
!pip install sec_api gradio
!pip install -U langchain langchain-community sentence-transformers faiss-gpu
!pip install flask-cors

from flask import Flask, request, jsonify
from flask_cors import CORS

import torch
from transformers import pipeline
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sec_api import QueryApi, ExtractorApi
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from unsloth import FastLanguageModel
from pyngrok import ngrok
import os

app = Flask(__name__)
CORS(app, origins=["http://localhost:3000"])

# Initialize global variables
db = None
retriever = None
current_ticker = None

# Initialize the summarization pipeline
device = 0 if torch.cuda.is_available() else -1
summarizer = pipeline("summarization", model="facebook/bart-large-cnn", device=device)

# SEC API key
sec_api_key = "338d568aa01901a6ba49d78b5b2ddb6039e23fd5df833a033b8427bee6489a98"

# Q&A prompt template
ft_prompt = """<|begin_of_text|><|start_header_id|>system<|end_header_id|>
Below is a user question, paired with retrieved context. Write a response that appropriately answers the question,
include specific details in your response. <|eot_id|>
<|start_header_id|>user<|end_header_id|>
### Question:
{}
### Context:
{}
<|eot_id|>
### Response: <|start_header_id|>assistant<|end_header_id|>
{}"""

# Load the Q&A model
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="/content/drive/MyDrive/l3_finagent/l3_finagent_step60",
    max_seq_length=2048,
    dtype=None,
    load_in_4bit=True,
)
FastLanguageModel.for_inference(model)

def summarize_in_batches(chunks, max_chunk_length=300, min_chunk_length=50):
    texts = [chunk.page_content for chunk in chunks]
    summaries = summarizer(
        texts,
        max_length=max_chunk_length,
        min_length=min_chunk_length,
        truncation=True
    )
    return "\n\n".join([summary['summary_text'] for summary in summaries])

def summarize_filing_data(text, max_chunk_length=1000):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=max_chunk_length, chunk_overlap=100, length_function=len
    )
    split_data = text_splitter.create_documents([text])
    return summarize_in_batches(split_data)

def get_filings(ticker):
    queryApi = QueryApi(api_key=sec_api_key)
    query = {
        "query": f"ticker:{ticker} AND formType:\"10-K\"",
        "from": "0",
        "size": "1",
        "sort": [{ "filedAt": { "order": "desc" } }]
    }
    filings = queryApi.get_filings(query)
    filing_url = filings["filings"][0]["linkToFilingDetails"]
    extractorApi = ExtractorApi(api_key=sec_api_key)
    onea_text = extractorApi.get_section(filing_url, "1A", "text")
    seven_text = extractorApi.get_section(filing_url, "7", "text")
    return onea_text + "\n\n" + seven_text

def inference(question, context):
    inputs = tokenizer(
        [ft_prompt.format(question, context, "")],
        return_tensors="pt"
    ).to("cuda")
    outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True, pad_token_id=tokenizer.eos_token_id)
    response = tokenizer.batch_decode(outputs)
    return response

def extract_response(text):
    text = text[0]
    start_token = "### Response: <|start_header_id|>assistant<|end_header_id|>"
    end_token = "<|eot_id|>"
    start_index = text.find(start_token) + len(start_token)
    end_index = text.find(end_token, start_index)
    if start_index == -1 or end_index == -1:
        return None
    return text[start_index:end_index].strip()

def initialize_vector_db(filing_data):
    modelPath = "BAAI/bge-large-en-v1.5"
    model_kwargs = {'device':'cuda'}
    encode_kwargs = {'normalize_embeddings': True}
    embeddings = HuggingFaceEmbeddings(
        model_name=modelPath,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=500,
        length_function=len,
        is_separator_regex=False
    )
    split_data = text_splitter.create_documents([filing_data])
    return FAISS.from_documents(split_data, embeddings)

def retrieve_context(query, retriever):
    retrieved_docs = retriever.invoke(query)
    return [doc.page_content for doc in retrieved_docs]

@app.route('/analyze', methods=['POST'])
def analyze_filing():
    global db, retriever, current_ticker

    data = request.get_json()
    if not data or 'ticker' not in data:
        return jsonify({'error': 'Please provide a ticker symbol'}), 400

    ticker = data['ticker']

    if ticker == current_ticker:
        return jsonify({
            'message': f'Filing for {ticker} has already been analyzed',
            'status': 'success'
        })

    try:
        filing_data = get_filings(ticker)
        summary = summarize_filing_data(filing_data)
        db = initialize_vector_db(filing_data)
        retriever = db.as_retriever()
        current_ticker = ticker

        return jsonify({
            'ticker': ticker,
            'summary': summary,
            'status': 'success'
        })
    except Exception as e:
        return jsonify({
            'error': str(e),
            'status': 'error'
        }), 500

@app.route('/ask', methods=['POST'])
def ask_question():
    global retriever, current_ticker

    data = request.get_json()
    if not data or 'question' not in data:
        return jsonify({'error': 'Please provide a question'}), 400

    if retriever is None or current_ticker is None:
        return jsonify({
            'error': 'Please analyze a filing first',
            'status': 'error'
        }), 400

    try:
        question = data['question']
        context = retrieve_context(question, retriever)
        resp = inference(question, context)
        parsed_response = extract_response(resp)

        return jsonify({
            'question': question,
            'answer': parsed_response,
            'status': 'success'
        })
    except Exception as e:
        return jsonify({
            'error': str(e),
            'status': 'error'
        }), 500

@app.route('/')
def home():
    """Root endpoint that shows API documentation"""
    return jsonify({
        'status': 'active',
        'version': '1.0',
        'endpoints': {
            '/': {
                'method': 'GET',
                'description': 'API documentation'
            },
            '/analyze': {
                'method': 'POST',
                'description': 'Analyze SEC filing for a given ticker',
                'parameters': {
                    'ticker': 'string (required) - Stock ticker symbol'
                },
                'example': {
                    'request': {'ticker': 'AAPL'},
                    'curl': 'curl -X POST http://your-ngrok-url/analyze -H "Content-Type: application/json" -d \'{"ticker": "AAPL"}\''
                }
            },
            '/ask': {
                'method': 'POST',
                'description': 'Ask questions about the analyzed filing',
                'parameters': {
                    'question': 'string (required) - Question about the filing'
                },
                'example': {
                    'request': {'question': 'What are the main risk factors?'},
                    'curl': 'curl -X POST http://your-ngrok-url/ask -H "Content-Type: application/json" -d \'{"question": "What are the main risk factors?"}\''
                }
            }
        }
    })

@app.errorhandler(404)
def not_found(e):
    """Handle 404 errors"""
    return jsonify({
        'error': 'Resource not found',
        'message': 'The requested URL was not found on the server. Please check the API documentation at the root endpoint (/)'
    }), 404

@app.errorhandler(500)
def server_error(e):
    """Handle 500 errors"""
    return jsonify({
        'error': 'Internal server error',
        'message': str(e)
    }), 500

# [Previous route definitions for /analyze and /ask remain the same...]

if __name__ == '__main__':
    # Start ngrok
    port = 5000
    public_url = ngrok.connect(port).public_url
    print(f' * ngrok tunnel "{public_url}" -> "http://127.0.0.1:{port}"')

    # Start Flask app
    app.run(port=port)